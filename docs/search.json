[
  {
    "objectID": "posts/palmer-penuins-blog-post/PalmerPenguins.html",
    "href": "posts/palmer-penuins-blog-post/PalmerPenguins.html",
    "title": "Visualizations and Table",
    "section": "",
    "text": "import pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\n#vectorizing categorical data via hot encoding\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nX_train_labs = X_train\nX_train_labs['Species'] = train['Species']\ndata_summary = X_train_labs.groupby('Species').aggregate('mean')\ndata_summary.head() #mean of the numeric features\n\n#Lia Notes: \n#1) Body mass of Gentoo very high compared to Adelie and Chinstrap\n\n#2) Culmin length Adelie different than Chinstrap and Gentoo\n\n#3) Chinstrap 100% on Island_Dream as compared to .37 for Adelie and 0 for Gentoo\n\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\n38.961111\n18.380556\n190.527778\n3722.916667\n8.861431\n-25.808814\n0.305556\n0.37963\n0.314815\n1.0\n0.101852\n0.898148\n0.481481\n0.518519\n\n\nChinstrap penguin (Pygoscelis antarctica)\n48.771429\n18.346429\n195.821429\n3739.732143\n9.331004\n-24.567075\n0.000000\n1.00000\n0.000000\n1.0\n0.178571\n0.821429\n0.553571\n0.446429\n\n\nGentoo penguin (Pygoscelis papua)\n47.133696\n14.926087\n216.739130\n5057.336957\n8.252573\n-26.145754\n1.000000\n0.00000\n0.000000\n1.0\n0.076087\n0.923913\n0.532609\n0.467391\nimport plotly.express as plotly\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Plot 3D scatter\nfig = plotly.scatter_3d(\n    X_train,\n    x='Culmen Length (mm)',\n    y='Body Mass (g)',\n    z='Flipper Length (mm)',\n    color='Species',\n    hover_name='Species'\n)\n\n\nfig.show()\n#bar plor of island population ratios\nbar= data_summary[[\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]].plot(\n    kind=\"barh\",\n    stacked=True,\n    figsize=(8, 6),\n    colormap=\"viridis\"\n)\n\n#labs :)\nbar.set_title(\"Island Population Ratios by Penguin Species\")\nbar.set_ylabel(\"Population Ratio\")\nbar.set_xlabel(\"Species\")\nbar.legend(title=\"Island\")\nplt.tight_layout()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "posts/palmer-penuins-blog-post/PalmerPenguins.html#model-training",
    "href": "posts/palmer-penuins-blog-post/PalmerPenguins.html#model-training",
    "title": "Visualizations and Table",
    "section": "Model Training",
    "text": "Model Training\nfeatures: Dream Island (categorical), Culmin Length (Numeric), and Body Mass (Numeric)\n\n## XGBooste Model\n\n\n\nfrom xgboost import XGBClassifier\n# XGBoost model\n\nxgb_model = XGBClassifier()"
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/logisticRegression-post/LogisticModel.html",
    "href": "posts/logisticRegression-post/LogisticModel.html",
    "title": "Logistic Regression Blog Post",
    "section": "",
    "text": "https://github.com/Lia-Smith/github.io/blob/main/posts/logisticRegression-post/logistic.py"
  },
  {
    "objectID": "posts/logisticRegression-post/LogisticModel.html#abstract",
    "href": "posts/logisticRegression-post/LogisticModel.html#abstract",
    "title": "Logistic Regression Blog Post",
    "section": "Abstract",
    "text": "Abstract\nThis blog posts implements logistic regression with momentum gradient descent. I perform experiments comparing the number of steps momentum and vanilla gradient descent take to converge for a randomly generated dataset and a breast cancer classification data set. Additionally, I experiment with overfitting to understand the affects it has on test data outcomes. Finally, I fit the breast cancer test data to see how well the model performs on the dataset.\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer"
  },
  {
    "objectID": "posts/logisticRegression-post/LogisticModel.html#gradient-descent-with-momentum",
    "href": "posts/logisticRegression-post/LogisticModel.html#gradient-descent-with-momentum",
    "title": "Logistic Regression Blog Post",
    "section": "Gradient Descent with Momentum",
    "text": "Gradient Descent with Momentum\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\ncross_entropy_loss_b = []\nsteps_b = np.arange(1, 101)\n\nfor i in range(100):\n    opt.step(X, y, alpha=0.1, beta=.9)\n    loss_momentum = LR.loss(X, y)\n    cross_entropy_loss_b.append(loss_momentum);\n\n# Plooooottting Both\n#momentum\nplt.plot(steps_b, cross_entropy_loss_b)\nplt.xlabel(\"Step\")\nplt.ylabel(\"Cross Entropy Loss\")\nplt.title(\"Loss Over Iterations with Momentum\")\nplt.grid(True)\nplt.show()\n\n# vanilla\nplt.plot(steps, cross_entropy_loss)\nplt.xlabel(\"Step\")\nplt.ylabel(\"Cross Entropy Loss\")\nplt.title(\"Loss Over Iterations Vanilla\")\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/logisticRegression-post/LogisticModel.html#comparison-of-vanilla-and-momentum-gradient-descent",
    "href": "posts/logisticRegression-post/LogisticModel.html#comparison-of-vanilla-and-momentum-gradient-descent",
    "title": "Logistic Regression Blog Post",
    "section": "Comparison of Vanilla and Momentum Gradient Descent",
    "text": "Comparison of Vanilla and Momentum Gradient Descent\nThis experiment is designed to illustrate the benefits of a more complex gradient descent algorithm. The above graphs illustrate convergence of the momemtum gradient descent occurs much more rapidly than the vanilla gradient descent. Momentum converges in about a 100 steps while vanilla gradient descent converges in about 2000 steps."
  },
  {
    "objectID": "posts/logisticRegression-post/LogisticModel.html#overfitting",
    "href": "posts/logisticRegression-post/LogisticModel.html#overfitting",
    "title": "Logistic Regression Blog Post",
    "section": "Overfitting",
    "text": "Overfitting\n\n# two identicle parameter data sets\nLR_overfit = LogisticRegression() \nopt_overfit = GradientDescentOptimizer(LR_overfit)\nX_train, y_train = classification_data(n_points = 50, noise = 0.5, p_dims = 200)\nX_test, y_test = classification_data(n_points = 50, noise = 0.5, p_dims = 200)\n\nadd_bias(X_test) #add ones column for intercept term\nadd_bias(X_train)\n\nfor i in range(1000):\n    opt_overfit.step(X_train, y_train, alpha=0.1, beta=.9)\n    loss = LR_overfit.loss(X_train,y_train);\n    \n    \ny_hat = 1/(1+torch.exp(-LR_overfit.score(X_train)))\nacc_train = torch.abs(y_hat-y_train-1).mean()\nprint(f\"training accuracy: {acc_train}\")\n\ny_hat = 1/(1+torch.exp(-LR_overfit.score(X_test)))\nacc_test = torch.abs(y_hat-y_test-1).mean()\nprint(f\"testing accuracy : {acc_test}\")\n\ntraining accuracy: 0.9998152256011963\ntesting accuracy : 0.8587407469749451"
  },
  {
    "objectID": "posts/logisticRegression-post/LogisticModel.html#overfitting-experiment",
    "href": "posts/logisticRegression-post/LogisticModel.html#overfitting-experiment",
    "title": "Logistic Regression Blog Post",
    "section": "Overfitting Experiment",
    "text": "Overfitting Experiment\nThis experiment demonstrates the relationship between model complexity and overfitting. As a model becomes more complex, it is likely to capture relationships within the training data that do not generalize to other data. Within this experiment, 150 features yielded 100% accuracy in the training set, but adversely affected the testing data, leading to a lower 83% accuracy."
  },
  {
    "objectID": "posts/logisticRegression-post/LogisticModel.html#introduction-to-the-data",
    "href": "posts/logisticRegression-post/LogisticModel.html#introduction-to-the-data",
    "title": "Logistic Regression Blog Post",
    "section": "Introduction to the Data",
    "text": "Introduction to the Data\nThe Breast Cancer dataset contains variables derived from digitalized images of a fine needle aspirate (FNA) of a breast mass. The features describe characteristics of the cell nuclie present in the image. This data was compiled from Wisconsin patients. The column diagnosis is a categorical variable with two levels, benign and malignant, which describe whether the mass is benign or malignant. Other variables are continuous variables delineating the various features of the digitized images such as worst_radius of the mass, etc. With this data, I aim to create a model to classify the data using logistic regression with momemtum in gradient descent. Unfortunately I was unable to find the cite the data is originally from.\nWolberg, William, W. Street, and Olvi Mangasarian. “Breast Cancer Wisconsin (Prognostic).” UCI Machine Learning Repository, 1995, https://doi.org/10.24432/C5GK50.\n\nimport pandas as pd\n#reading in the data\nbreast_cancer = pd.read_csv(\"C:/Users/liapu/OneDrive/Desktop/Fall 2024/breast-cancer.csv\")\nbreast_cancer.head()\n\n#turning pd data frames into torch tensors\ntarget = breast_cancer['diagnosis'].map({\"M\": 1, \"B\": 0})\ndata = breast_cancer.drop('diagnosis', axis = 1)\ndata = data.drop('id', axis = 1)\n\nX = torch.tensor(data.values, dtype=torch.float32)\ny = torch.tensor(target.values, dtype = torch.float32)\ndata.head()\n\n\n\n\n\n\n\n\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\nsymmetry_mean\nfractal_dimension_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows × 30 columns\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n# 60% train, 20% val, 20% test (using skit learn :O )\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp)\n\nadd_bias(X_train);\nadd_bias(X_test);\nadd_bias(X_val);\n\n\n#no momentum\ncross_entropy_loss = []\nsteps = np.arange(1, 1001)\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nfor i in range(1000):\n    opt.step(X_train, y_train, alpha=0.1, beta=0)\n    loss = LR.loss(X_train, y_train)\n    cross_entropy_loss.append(loss);\n\n\n #momentum\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\ncross_entropy_loss_b = []\nsteps_b = np.arange(1, 1001)\n\nfor i in range(1000):\n    opt.step(X_train, y_train, alpha=0.01, beta=.9)\n    loss = LR.loss(X_train, y_train)\n    cross_entropy_loss_b.append(loss);\n\n\n# Plooooottting Both\n#momentum\nplt.plot(steps_b, cross_entropy_loss_b)\nplt.xlabel(\"Step\")\nplt.ylabel(\"Cross Entropy Loss\")\nplt.title(\"Loss Over Iterations with Momentum\")\nplt.grid(True)\nplt.show()\n\n# vanilla\nplt.plot(steps, cross_entropy_loss)\nplt.xlabel(\"Step\")\nplt.ylabel(\"Cross Entropy Loss\")\nplt.title(\"Loss Over Iterations wthout Momentum\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#validation loss w momentum/ no momentum\n #momentum\nLR_val = LogisticRegression() \nopt_val = GradientDescentOptimizer(LR_val)\n\ncross_entropy_loss_val = []\nsteps_val = np.arange(1, 1001)\n\nfor i in range(1000):\n    opt_val.step(X_val, y_val, alpha=0.01, beta=.9)\n    loss_val = LR_val.loss(X_val, y_val)\n    cross_entropy_loss_val.append(loss_val);\n\n\nLR_val_van = LogisticRegression() \nopt_val_van = GradientDescentOptimizer(LR_val_van)\n\ncross_entropy_loss_val_van = []\nsteps_val_van = np.arange(1, 1001)\n\nfor i in range(1000):\n    opt_val_van.step(X_val, y_val, alpha=0.01, beta=.9)\n    loss_val_van = LR_val_van.loss(X_val, y_val)\n    cross_entropy_loss_val_van.append(loss_val_van);\n\n\n# Plooooottting Both\n#momentum\nplt.plot(steps_val, cross_entropy_loss_val)\nplt.xlabel(\"Step\")\nplt.ylabel(\"Cross Entropy Loss\")\nplt.title(\"Loss Over Iterations with Momentum\")\nplt.grid(True)\nplt.show()\n\n# vanilla\nplt.plot(steps_val_van, cross_entropy_loss_val_van)\nplt.xlabel(\"Step\")\nplt.ylabel(\"Cross Entropy Loss\")\nplt.title(\"Loss Over Iterations wthout Momentum\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#test set evaluation/model loss\nloss = LR.loss(X_test, y_test)\nprint(f\"loss : {loss}\")\n#probabillity\ny_hat = 1/(1+torch.exp(-LR.score(X_test)))\n\n#make 0 or 1 \ny_pred = (y_hat &gt;= 0.5).float()\n\n#check for correct classification + mean for %correct\naccuracy = (y_pred == y_test).float().mean()\n\nprint(f\"accuracy : {accuracy}\")\n\nloss : 0.14749480783939362\naccuracy : 0.9666666388511658"
  },
  {
    "objectID": "posts/logisticRegression-post/LogisticModel.html#discussion",
    "href": "posts/logisticRegression-post/LogisticModel.html#discussion",
    "title": "Logistic Regression Blog Post",
    "section": "Discussion",
    "text": "Discussion\nIn this notebook, I utilized my own implementation of logistic regression to compare gradient descent with and without momentum, simulate overfitting to the data, and fit the model to an actual dataset. My model performs extremely well on this dataset, without being too computationally expensive. Logistic regression is a simple machine learning algorithm that performs well on binary classification tasks."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog :D"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lia Smith CSCI 0451 Blog",
    "section": "",
    "text": "Advanced Optimizers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKernel Blog Post\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizations and Table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression Blog Post\n\n\n\n\n\nA blog post implementing logistic regression.\n\n\n\n\n\nApr 9, 2025\n\n\nLia Smith\n\n\n\n\n\n\n\n\n\n\n\n\nPerceptron Blog Post\n\n\n\n\n\nA blog post implementing the Perceptron Algorithm.\n\n\n\n\n\nApr 3, 2025\n\n\nLia Smith\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/kernal-methods-post/sparse-kernel-machines.html",
    "href": "posts/kernal-methods-post/sparse-kernel-machines.html",
    "title": "Kernel Blog Post",
    "section": "",
    "text": "from logistic import KernelLogisticRegression\n\nlogistic implementation link: https://github.com/Lia-Smith/github.io/blob/main/posts/kernal-methods-post/logistic.py\n\n\nIn this kernel logistic regression blog post, I expanded upon logistic regression by kernelizing the score and loss functions. Additionally, I explored the higher volatility that kernel function can bring to overfitting with gamma and lambda hyperparameters.\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n   \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    # X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n   \n    X = X - X.mean(dim = 0, keepdim = True)\n    return X, y\n\n\ndef plot_classification_data(X, y, ax):\n    assert X.shape[1] == 2, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -1, vmax = 2, alpha = 0.8, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = classification_data(n_points = 100, noise = 0.4)\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\n\ndef rbf_kernel(X_1, X_2, gamma=1.0):\n    return torch.exp(-gamma*torch.cdist(X_1, X_2)**2)\n#default kernel, encouraged to use others NOTE \n\n\n\nKR = KernelLogisticRegression(rbf_kernel, lam=0.1, gamma=1)\nKR.fit(X, y, m_epochs=50000, lr=0.0001)\n(1.0 * (KR.a &gt; 0.001)).mean()\n\ntensor(0.1900)\n\n\n\nix = torch.abs(KR.a) &gt; 0.001\n\nx1 = torch.linspace(X[:,0].min() - 0.2, X[:,0].max() + 0.2, 101)\nx2 = torch.linspace(X[:,1].min() - 0.2, X[:,1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim = 1)\n\npreds = KR.score(X_)\npreds = 1.0*torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1, 1)\nax.contourf(X1, X2, preds, origin = \"lower\", cmap = \"BrBG\",\nvmin = 2*preds.min() - preds.max(), vmax = 2*preds.max() - preds.min()\n)\nplot_classification_data(X, y, ax)\nplt.scatter(X[ix, 0],X[ix, 1], facecolors = \"none\", edgecolors = \"black\")\n# ax.scatter(X[ix, 0],X[ix, 1], facecolors = \"none\", edgecolors = \"black\")\n\n\n\n\n\n\n\n\n\n\n\nThis section demonstrates basic functionality of the sparse kernel logistic regression implementation. I demontrate here how varying the regularization parameter λ and kernel width γ affects model sparsity and decision boundaries.\n\n## lambda\n\nlambdas = [0.01, 0.1, 1, 10, 100, 1000]\nnonzero_weights = []\n\nfor lam in lambdas:\n    KR = KernelLogisticRegression(rbf_kernel, lam=lam, gamma=1)\n    KR.fit(X, y, m_epochs=50000, lr=0.0001)\n    nonzero = (KR.a &gt; 0.001).sum()\n    nonzero_weights.append(nonzero)\n\nplt.plot(lambdas, nonzero_weights, marker='o')\nplt.xscale('log')\nplt.xlabel('Lambda (λ)')\nplt.ylabel('Number of nonzero weights')\nplt.title('Effect of λ on sparsity')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\ndef plot_decision_boundary(model, X, y, h=0.02, title=None):\n    \"\"\"\n    Plots the decision boundary learned by a KernelLogisticRegression model.\n\n    Args:\n        model: Trained KernelLogisticRegression model.\n        X (torch.Tensor): Input data of shape (n, 2)\n        y (torch.Tensor): Labels of shape (n,)\n        h (float): Step size in the mesh.\n        title (str): Optional title for the plot.\n    \"\"\"\n    #torch to numpy\n    X_np = X.detach().numpy()\n    y_np = y.detach().numpy()\n\n    x_min, x_max = X_np[:, 0].min() - 1, X_np[:, 0].max() + 1\n    y_min, y_max = X_np[:, 1].min() - 1, X_np[:, 1].max() + 1\n\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    grid = np.c_[xx.ravel(), yy.ravel()]\n    grid_tensor = torch.from_numpy(grid).float()\n\n    # Get scores + sigmoid\n    with torch.no_grad():\n        scores = model.score(grid_tensor)\n        probs = torch.sigmoid(scores)\n\n    Z = probs.reshape(xx.shape)\n\n    # Plotlolol\n    plt.contourf(xx, yy, Z, levels=[0, 0.5, 1], alpha=0.3, colors=[\"#FFAAAA\", \"#AAAAFF\"])\n    plt.contour(xx, yy, Z, levels=[0.5], colors=\"k\", linewidths=1)\n\n    # Scatter original data\n    plt.scatter(X_np[y_np==0][:, 0], X_np[y_np==0][:, 1], c='red', label='Class 0', edgecolors='k')\n    plt.scatter(X_np[y_np==1][:, 0], X_np[y_np==1][:, 1], c='blue', label='Class 1', edgecolors='k')\n\n    plt.legend()\n    plt.title(title or \"Decision Boundary\")\n    plt.xlabel(\"x1\")\n    plt.ylabel(\"x2\")\n    plt.grid(True)\n    plt.show()\n\n\n## gamma\ngammas = [0.1, 1, 10]\nfor gamma in gammas:\n    KR = KernelLogisticRegression(rbf_kernel, lam=0.1, gamma=gamma)\n    KR.fit(X, y, m_epochs=50000, lr=0.0001)\n    plot_decision_boundary(KR, X, y, title=f'Decision Boundary with gamma={gamma}')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs the value of Gamma Increases, the fluidity of the decision boundry increases.\n\n\n\nI created two sets of data (training and testing) and show how the wrong choice of γ can lead to overfitting while visualizing both the decision boundary and performance using ROC curves.\n\nfrom sklearn.metrics import roc_curve, auc\ndef plot_roc_curves(model, X_train, y_train, X_test, y_test):\n    y_train_pred = model.score(X_train).detach().numpy()\n    y_test_pred = model.score(X_test).detach().numpy()\n\n    fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred)\n    fpr_test, tpr_test, _ = roc_curve(y_test, y_test_pred)\n\n    auc_train = auc(fpr_train, tpr_train)\n    auc_test = auc(fpr_test, tpr_test)\n\n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr_train, tpr_train, label=f\"Train ROC (AUC = {auc_train:.2f})\", color='blue')\n    plt.plot(fpr_test, tpr_test, label=f\"Test ROC (AUC = {auc_test:.2f})\", color='red')\n    plt.plot([0, 1], [0, 1], 'k--', label=\"Random\")\n    plt.title(\"ROC Curves: Overfit Model (high λ)\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=200, noise=0.4)\nX_test, y_test = make_moons(n_samples = 200, noise= 0.4)\n#np to tensor\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.float32)\n\n#model\nKR = KernelLogisticRegression(rbf_kernel, lam=0.5, gamma=100)\nKR.fit(X, y, m_epochs=50000, lr=0.0001)\nplot_decision_boundary(KR, X, y, title = \"Overfitting with Make Moons Data\")\n\nplot_roc_curves(KR, X, y, X_test, y_test)"
  },
  {
    "objectID": "posts/kernal-methods-post/sparse-kernel-machines.html#changing-gamma",
    "href": "posts/kernal-methods-post/sparse-kernel-machines.html#changing-gamma",
    "title": "Large Lambda",
    "section": "Changing Gamma",
    "text": "Changing Gamma\n\nKR = KernelLogisticRegression(rbf_kernel, lam=.1, gamma=10)\nKR.fit(X, y, m_epochs=50000, lr=0.0001)\n(1.0 * (KR.a &gt; 0.001)).mean()\n\ntensor(0.)\n\n\n\nix = torch.abs(KR.a) &gt; 0.001\n\nx1 = torch.linspace(X[:,0].min() - 0.2, X[:,0].max() + 0.2, 101)\nx2 = torch.linspace(X[:,1].min() - 0.2, X[:,1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim = 1)\n\npreds = KR.score(X_)\npreds = 1.0*torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1, 1)\nax.contourf(X1, X2, preds, origin = \"lower\", cmap = \"BrBG\",\nvmin = 2*preds.min() - preds.max(), vmax = 2*preds.max() - preds.min()\n)\nplot_classification_data(X, y, ax)\nplt.scatter(X[ix, 0],X[ix, 1], facecolors = \"none\", edgecolors = \"black\")\n# ax.scatter(X[ix, 0],X[ix, 1], facecolors = \"none\", edgecolors = \"black\")"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/Overfitting-Overparameterization-blog-post/Overfitting.html",
    "href": "posts/Overfitting-Overparameterization-blog-post/Overfitting.html",
    "title": "Part 0",
    "section": "",
    "text": "from LinearRegressionOverfit import OverParameterizedLinearRegressionOptimizer, MyLinearRegression, RandomFeatures\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nw = (X^TX)^-1  X^T *y This matrix becomes uninvertable when there are more features than data points, because X^TX becomes a linearly dependent matrix that isn’t row reduceable. If (X^TX) isn’t invertable, then we cannot take the inverse of the matrix and cannot solve for w.\n\ndef add_bias(X):\n        \"\"\"\n        Adds a column of ones to X to account for the intercept term :D.\n\n        Arguments:\n            X, torch.Tensor: The feature matrix of size (n, p)\n\n        Returns:\n            X_with_bias, torch.Tensor: New matrix of size (n, p+1)\n        \"\"\"\n        n = X.size(0)\n        ones = torch.ones(n, 1) # make da onessss \n        return torch.cat((X, ones), dim=1)\n\n\n## Part A checks\nLR = MyLinearRegression()\nopt = OverParameterizedLinearRegressionOptimizer(LR)\n\nX = torch.tensor(np.linspace(-3, 3, 100).reshape(-1, 1), dtype = torch.float64)\ny = X**4 - 4*X + torch.normal(0, 5, size=X.shape)\n\nplt.scatter(X, y, color='darkgrey', label='Data')\nopt.fit(X, y)\n\n\n\n\n\n\n\n\n\n\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[5], line 26\n     23 grid_inputs = torch.cat((grid_inputs, torch.ones((grid_inputs.shape[0], 1), dtype=torch.float64)), 1)\n     25 # Predict\n---&gt; 26 Z = LR.predict(grid_inputs).detach().numpy()\n     28 # Plot the decision boundary where prediction = 0.5\n     29 plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n\nFile c:\\Users\\liapu\\OneDrive\\Documents\\GitHub\\github.io\\posts\\Overfitting-Overparameterization-blog-post\\LinearRegressionOverfit.py:49, in MyLinearRegression.predict(self, X)\n     33 def predict(self, X):\n     34     \"\"\"\n     35     Compute the scores for each data point in the feature matrix X. \n     36     The formula for the ith entry of score is score[i] = &lt;self.w, x[i]&gt;. \n   (...)\n     47         score torch.Tensor: vector of scores. score.size() = (n,)\n     48     \"\"\"\n---&gt; 49     score = self.score(X)\n     50     return score\n\nFile c:\\Users\\liapu\\OneDrive\\Documents\\GitHub\\github.io\\posts\\Overfitting-Overparameterization-blog-post\\LinearRegressionOverfit.py:28, in LinearModel.score(self, X)\n     24     self.w = torch.rand((X.size()[1]), dtype=X.dtype)\n     27 # your computation here: compute the vector of scores s\n---&gt; 28 s = X @ self.w\n     29 return s\n\nRuntimeError: expected scalar type Double but found Float"
  },
  {
    "objectID": "posts/kernal-methods-post/sparse-kernel-machines.html#part-a-basic-experiments",
    "href": "posts/kernal-methods-post/sparse-kernel-machines.html#part-a-basic-experiments",
    "title": "Kernel Blog Post",
    "section": "",
    "text": "This section demonstrates basic functionality of the sparse kernel logistic regression implementation. I demontrate here how varying the regularization parameter λ and kernel width γ affects model sparsity and decision boundaries.\n\n## lambda\n\nlambdas = [0.01, 0.1, 1, 10, 100, 1000]\nnonzero_weights = []\n\nfor lam in lambdas:\n    KR = KernelLogisticRegression(rbf_kernel, lam=lam, gamma=1)\n    KR.fit(X, y, m_epochs=50000, lr=0.0001)\n    nonzero = (KR.a &gt; 0.001).sum()\n    nonzero_weights.append(nonzero)\n\nplt.plot(lambdas, nonzero_weights, marker='o')\nplt.xscale('log')\nplt.xlabel('Lambda (λ)')\nplt.ylabel('Number of nonzero weights')\nplt.title('Effect of λ on sparsity')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\ndef plot_decision_boundary(model, X, y, h=0.02, title=None):\n    \"\"\"\n    Plots the decision boundary learned by a KernelLogisticRegression model.\n\n    Args:\n        model: Trained KernelLogisticRegression model.\n        X (torch.Tensor): Input data of shape (n, 2)\n        y (torch.Tensor): Labels of shape (n,)\n        h (float): Step size in the mesh.\n        title (str): Optional title for the plot.\n    \"\"\"\n    #torch to numpy\n    X_np = X.detach().numpy()\n    y_np = y.detach().numpy()\n\n    x_min, x_max = X_np[:, 0].min() - 1, X_np[:, 0].max() + 1\n    y_min, y_max = X_np[:, 1].min() - 1, X_np[:, 1].max() + 1\n\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    grid = np.c_[xx.ravel(), yy.ravel()]\n    grid_tensor = torch.from_numpy(grid).float()\n\n    # Get scores + sigmoid\n    with torch.no_grad():\n        scores = model.score(grid_tensor)\n        probs = torch.sigmoid(scores)\n\n    Z = probs.reshape(xx.shape)\n\n    # Plotlolol\n    plt.contourf(xx, yy, Z, levels=[0, 0.5, 1], alpha=0.3, colors=[\"#FFAAAA\", \"#AAAAFF\"])\n    plt.contour(xx, yy, Z, levels=[0.5], colors=\"k\", linewidths=1)\n\n    # Scatter original data\n    plt.scatter(X_np[y_np==0][:, 0], X_np[y_np==0][:, 1], c='red', label='Class 0', edgecolors='k')\n    plt.scatter(X_np[y_np==1][:, 0], X_np[y_np==1][:, 1], c='blue', label='Class 1', edgecolors='k')\n\n    plt.legend()\n    plt.title(title or \"Decision Boundary\")\n    plt.xlabel(\"x1\")\n    plt.ylabel(\"x2\")\n    plt.grid(True)\n    plt.show()\n\n\n## gamma\ngammas = [0.1, 1, 10]\nfor gamma in gammas:\n    KR = KernelLogisticRegression(rbf_kernel, lam=0.1, gamma=gamma)\n    KR.fit(X, y, m_epochs=50000, lr=0.0001)\n    plot_decision_boundary(KR, X, y, title=f'Decision Boundary with gamma={gamma}')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs the value of Gamma Increases, the fluidity of the decision boundry increases."
  },
  {
    "objectID": "posts/kernal-methods-post/sparse-kernel-machines.html#part-b-demonstrating-overfitting",
    "href": "posts/kernal-methods-post/sparse-kernel-machines.html#part-b-demonstrating-overfitting",
    "title": "Kernel Blog Post",
    "section": "",
    "text": "I created two sets of data (training and testing) and show how the wrong choice of γ can lead to overfitting while visualizing both the decision boundary and performance using ROC curves.\n\nfrom sklearn.metrics import roc_curve, auc\ndef plot_roc_curves(model, X_train, y_train, X_test, y_test):\n    y_train_pred = model.score(X_train).detach().numpy()\n    y_test_pred = model.score(X_test).detach().numpy()\n\n    fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred)\n    fpr_test, tpr_test, _ = roc_curve(y_test, y_test_pred)\n\n    auc_train = auc(fpr_train, tpr_train)\n    auc_test = auc(fpr_test, tpr_test)\n\n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr_train, tpr_train, label=f\"Train ROC (AUC = {auc_train:.2f})\", color='blue')\n    plt.plot(fpr_test, tpr_test, label=f\"Test ROC (AUC = {auc_test:.2f})\", color='red')\n    plt.plot([0, 1], [0, 1], 'k--', label=\"Random\")\n    plt.title(\"ROC Curves: Overfit Model (high λ)\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=200, noise=0.4)\nX_test, y_test = make_moons(n_samples = 200, noise= 0.4)\n#np to tensor\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.float32)\n\n#model\nKR = KernelLogisticRegression(rbf_kernel, lam=0.5, gamma=100)\nKR.fit(X, y, m_epochs=50000, lr=0.0001)\nplot_decision_boundary(KR, X, y, title = \"Overfitting with Make Moons Data\")\n\nplot_roc_curves(KR, X, y, X_test, y_test)"
  },
  {
    "objectID": "posts/advanced-optimization-post/optimization.html",
    "href": "posts/advanced-optimization-post/optimization.html",
    "title": "Advanced Optimizers",
    "section": "",
    "text": "Optimizers link: https://github.com/Lia-Smith/github.io/blob/main/posts/advanced-optimization-post/logistic.py"
  },
  {
    "objectID": "posts/advanced-optimization-post/optimization.html#breast-cancer-dataset-for-testing",
    "href": "posts/advanced-optimization-post/optimization.html#breast-cancer-dataset-for-testing",
    "title": "Advanced Optimizers",
    "section": "Breast Cancer Dataset for Testing",
    "text": "Breast Cancer Dataset for Testing\n\nimport pandas as pd\n#reading in the data\nbreast_cancer = pd.read_csv(\"C:/Users/liapu/OneDrive/Desktop/Fall 2024/breast-cancer.csv\")\nbreast_cancer.head()\n\n#turning pd data frames into torch tensors\ntarget = breast_cancer['diagnosis'].map({\"M\": 1, \"B\": 0})\ndata = breast_cancer.drop('diagnosis', axis = 1)\ndata = data.drop('id', axis = 1)\n\nX = torch.tensor(data.values, dtype=torch.float32)\ny = torch.tensor(target.values, dtype = torch.float32)\nadd_bias(X)\ndata.head()\n\n\n\n\n\n\n\n\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\nsymmetry_mean\nfractal_dimension_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows × 30 columns"
  },
  {
    "objectID": "posts/advanced-optimization-post/optimization.html#newtons-method",
    "href": "posts/advanced-optimization-post/optimization.html#newtons-method",
    "title": "Advanced Optimizers",
    "section": "Newton’s Method",
    "text": "Newton’s Method\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\n\nalphas = [0.00001, 0.00005, 0.00002]\ncolors = ['blue', 'purple', 'red']\nlabels = [f\"alpha = {a}\" for a in alphas]\n\nsteps = np.arange(1, 1001) \nall_losses = []\n\nfor alpha in alphas:\n    LR = LogisticRegression()\n    opt = NewtonOptimizer(LR)\n    losses = []\n\n    for i in range(1000):\n        opt.step(X, y, alpha=alpha)\n        loss = LR.loss(X, y).item()\n        losses.append(loss)\n    \n    all_losses.append(losses)\n\n# Ploooting all\n# Create subplots\nfig, axs = plt.subplots(1, 3, figsize=(18, 5), sharey=True)\n\nfor i, (losses, color, label) in enumerate(zip(all_losses, colors, labels)):\n    axs[i].plot(steps, losses, color=color)\n    axs[i].set_title(label)\n    axs[i].set_xlabel(\"Step\")\n    if i == 0:\n        axs[i].set_ylabel(\"Cross Entropy Loss\")\n    axs[i].grid(True)\n\nfig.suptitle(\"Newton's Method: Loss Convergence for Different Learning Rates\", fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\nThe Newton Method of Gradient Descent seems to be able to converge at a good solution quickly, but take a bit more time to find an extremely optimal solution to the weight vector."
  },
  {
    "objectID": "posts/advanced-optimization-post/optimization.html#adaptive-momentum",
    "href": "posts/advanced-optimization-post/optimization.html#adaptive-momentum",
    "title": "Advanced Optimizers",
    "section": "Adaptive Momentum",
    "text": "Adaptive Momentum\n\n# Initializer\nLR = LogisticRegression() \nopt = AdamOptimizer(LR)\nsteps = np.arange(1, 101)\ncross_entropy_loss_adam = []\n\n#100 steps\nfor i in range(100):\n    opt.step(X, y, alpha=0.01, beta1=0.9, beta2=0.999, eps=1e-8, batch_size=32)\n    loss = LR.loss(X, y).item()\n    cross_entropy_loss_adam.append(loss)\n\n# Plot the loss curve\nplt.plot(steps, cross_entropy_loss_adam)\nplt.xlabel(\"Step\")\nplt.ylabel(\"Cross Entropy Loss\")\nplt.title(\"Loss Over Iterations with Adam Optimizer\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThis method converges so quickly and does such a good job of finding an optimal solution. I’m extremely impressed with the adaptive momentum algorithm."
  },
  {
    "objectID": "posts/advanced-optimization-post/optimization.html#newtons-method-versus-adaptive-momentum",
    "href": "posts/advanced-optimization-post/optimization.html#newtons-method-versus-adaptive-momentum",
    "title": "Advanced Optimizers",
    "section": "Newton’s Method Versus Adaptive Momentum:",
    "text": "Newton’s Method Versus Adaptive Momentum:\nReiterating what I said previously, Newton’s method seems to have a bit of a faster initial convergence, but then gets stuck at a suboptimal local minima, while adaptive momentum seems to find a quite ideal solution extremely quickly. Additionally, the hessian matrix is more computationally expensive than adaptive momentum, making adam preferable. Maybe I just need to find a better learning rate."
  },
  {
    "objectID": "posts/advanced-optimization-post/optimization.html#conclusion",
    "href": "posts/advanced-optimization-post/optimization.html#conclusion",
    "title": "Advanced Optimizers",
    "section": "Conclusion",
    "text": "Conclusion\nState of the art Optimizer algorithms are crucial for conserving runtime in machine learning while obtaining optimal weights for mapping the data. Adaptive Momentum really seems to take the cake with runtime, and convergence."
  },
  {
    "objectID": "posts/advanced-optimization-post/optimization.html#abstract",
    "href": "posts/advanced-optimization-post/optimization.html#abstract",
    "title": "Advanced Optimizers",
    "section": "Abstract:",
    "text": "Abstract:\nThis post explores advanced optimization methods for logistic regression: Newton’s Method and the Adam optimizer. I implement both from scratch, compare them and evaluate their convergence behavior and efficiency on a binary classification task.\n\nfrom logistic import AdamOptimizer, LogisticRegression, NewtonOptimizer, GradientDescentOptimizer\n\n\nimport time\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef run_experiment(model_class, optimizer_class, optimizer_args, X, y, steps=50):\n    model = model_class()\n    optimizer = optimizer_class(model)\n    losses = []\n    times = []\n    \n    for i in range(steps):\n        start = time.time()\n        optimizer.step(X, y, **optimizer_args)\n        elapsed = time.time() - start\n        loss = model.loss(X, y).item()\n        losses.append(loss)\n        times.append(elapsed)\n    \n    return losses, np.cumsum(times)\n\n\nimport torch\ndef add_bias(X):\n        \"\"\"\n        Adds a column of ones to X to account for the intercept term :D.\n\n        Arguments:\n            X, torch.Tensor: The feature matrix of size (n, p)\n\n        Returns:\n            X_with_bias, torch.Tensor: New matrix of size (n, p+1)\n        \"\"\"\n        n = X.size(0)\n        ones = torch.ones(n, 1) # make da onessss \n        return torch.cat((X, ones), dim=1)"
  },
  {
    "objectID": "posts/kernal-methods-post/sparse-kernel-machines.html#abstract",
    "href": "posts/kernal-methods-post/sparse-kernel-machines.html#abstract",
    "title": "Kernel Blog Post",
    "section": "",
    "text": "In this kernel logistic regression blog post, I expanded upon logistic regression by kernelizing the score and loss functions. Additionally, I explored the higher volatility that kernel function can bring to overfitting with gamma and lambda hyperparameters.\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n   \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    # X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n   \n    X = X - X.mean(dim = 0, keepdim = True)\n    return X, y\n\n\ndef plot_classification_data(X, y, ax):\n    assert X.shape[1] == 2, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -1, vmax = 2, alpha = 0.8, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = classification_data(n_points = 100, noise = 0.4)\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\n\ndef rbf_kernel(X_1, X_2, gamma=1.0):\n    return torch.exp(-gamma*torch.cdist(X_1, X_2)**2)\n#default kernel, encouraged to use others NOTE \n\n\n\nKR = KernelLogisticRegression(rbf_kernel, lam=0.1, gamma=1)\nKR.fit(X, y, m_epochs=50000, lr=0.0001)\n(1.0 * (KR.a &gt; 0.001)).mean()\n\ntensor(0.1900)\n\n\n\nix = torch.abs(KR.a) &gt; 0.001\n\nx1 = torch.linspace(X[:,0].min() - 0.2, X[:,0].max() + 0.2, 101)\nx2 = torch.linspace(X[:,1].min() - 0.2, X[:,1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim = 1)\n\npreds = KR.score(X_)\npreds = 1.0*torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1, 1)\nax.contourf(X1, X2, preds, origin = \"lower\", cmap = \"BrBG\",\nvmin = 2*preds.min() - preds.max(), vmax = 2*preds.max() - preds.min()\n)\nplot_classification_data(X, y, ax)\nplt.scatter(X[ix, 0],X[ix, 1], facecolors = \"none\", edgecolors = \"black\")\n# ax.scatter(X[ix, 0],X[ix, 1], facecolors = \"none\", edgecolors = \"black\")"
  },
  {
    "objectID": "posts/perceptron-post/PerceptronPost.html",
    "href": "posts/perceptron-post/PerceptronPost.html",
    "title": "Perceptron Blog Post",
    "section": "",
    "text": "In this blogpost, I am implementing the perceptron algorithm On both linearly divisible data and non-linearly divisible data. I am exploring what it means for an algorithm to learn optimal weights with a loss function and weight update. Finally, I am utilizing an outside dataset to understand how well a perceptron can perform on it.\nperceptron.py link: https://github.com/Lia-Smith/github.io/blob/main/posts/perceptron-post/perceptron.py\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\n\nimport torch\nimport pandas\nfrom matplotlib import pyplot as plt\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    \n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\n\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n\nimport matplotlib.pyplot as plt\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nThe model converges on the linearly separable data, therefore we know that the perceptron algorithm is working!\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X, y, ax)\n\nw_0 = torch.Tensor([1, -1, 0])\nw_1 = torch.Tensor([1,  1, -1]) \n\ndraw_line(w_0, 0, 1, ax, color = \"black\", linestyle = \"dashed\", label = r\"$w^{(0)}$\")\ndraw_line(w_1, 0, 1, ax, color = \"black\", label = r\"$w^{(1)}$\")\n\nl = ax.legend(ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n\n    if local_loss &gt; 0:\n        opt.step(x_i, y_i)\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2*(y[i].item())-1]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\n\n\n\n\n\nThe following training demonstrates the slow convergence of the perceptron into a linear separation of the two classes.\n\n\n\n\nX, y = perceptron_data(n_points = 300, noise = .35)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\nit = 0\nwhile loss &gt; 0 or it &gt;=1001:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    if local_loss &gt; 0:\n        it +=1\n        opt.step(x_i, y_i)\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0 and it%166==0:\n        print(it)\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2*(y[i].item())-1]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n166\n332\n498\n664\n830\n996\n\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[44], line 20\n     18 it = 0\n     19 while loss &gt; 0 or it &gt;=1001:\n---&gt; 20     ax = axarr.ravel()[current_ax]\n     22     # save the old value of w for plotting later\n     23     old_w = torch.clone(p.w)\n\nIndexError: index 6 is out of bounds for axis 0 with size 6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\n#reading in the data\nbreast_cancer = pd.read_csv(\"C:/Users/liapu/OneDrive/Desktop/Fall 2024/breast-cancer.csv\")\nbreast_cancer.head()\n\n#turning pd data frames into torch tensors\ntarget = breast_cancer['diagnosis'].map({\"M\": 1, \"B\": 0})\ndata = breast_cancer.drop('diagnosis', axis = 1)\ndata = data.drop('id', axis = 1)\n\nX = torch.tensor(data.values, dtype=torch.float32)\ny = torch.tensor(target.values, dtype = torch.float32)\ndata.head()\n\n\n\n\n\n\n\n\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\nsymmetry_mean\nfractal_dimension_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows × 30 columns\n\n\n\n\n# Initialize\ntorch.manual_seed(1234567)\np = Perceptron()\nopt = PerceptronOptimizer(p)\nn = X.shape[0]\n\n# Training loop parameters\nmax_steps = 1000\nloss_vec = []\nit = 0\nloss = p.loss(X, y).item()\n\n# Training loop\nwhile loss &gt; 0 and it &lt; max_steps:\n    i = torch.randint(n, size=(1,))\n    x_i = X[[i], :]\n    y_i = y[i]\n\n    local_loss = p.loss(x_i, y_i).item()\n\n    if local_loss &gt; 0:\n        opt.step(x_i, y_i)\n\n    # Update total loss and iteration counter\n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n    it += 1\n\n    # Print progress every 100 steps\n    if it % 100 == 0:\n        print(f\"Step {it}: loss = {loss:.4f}\")\n\n# Final message\nprint(f\"\\nTraining completed in {it} steps with final loss = {loss:.4f}\")\n\nStep 100: loss = 0.3620\nStep 200: loss = 0.3568\nStep 300: loss = 0.3322\nStep 400: loss = 0.1916\nStep 500: loss = 0.3023\nStep 600: loss = 0.6186\nStep 700: loss = 0.0914\nStep 800: loss = 0.1318\nStep 900: loss = 0.4429\nStep 1000: loss = 0.2127\n\nTraining completed in 1000 steps with final loss = 0.2127\n\n\n\nimport matplotlib.pyplot as plt\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nI don’t beleive this data is linearly separable, since it always seems to be occilating above .9, suggestting that .9 is the best we can do with this algorithm in terms of cross entropy loss."
  },
  {
    "objectID": "posts/perceptron-post/PerceptronPost.html#experiment-one",
    "href": "posts/perceptron-post/PerceptronPost.html#experiment-one",
    "title": "Perceptron Blog Post",
    "section": "",
    "text": "torch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n\n    if local_loss &gt; 0:\n        opt.step(x_i, y_i)\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2*(y[i].item())-1]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\n\n\n\n\n\nThe following training demonstrates the slow convergence of the perceptron into a linear separation of the two classes."
  },
  {
    "objectID": "posts/perceptron-post/PerceptronPost.html#experiment-two",
    "href": "posts/perceptron-post/PerceptronPost.html#experiment-two",
    "title": "Perceptron Blog Post",
    "section": "",
    "text": "X, y = perceptron_data(n_points = 300, noise = .35)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\nit = 0\nwhile loss &gt; 0 or it &gt;=1001:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n    if local_loss &gt; 0:\n        it +=1\n        opt.step(x_i, y_i)\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0 and it%166==0:\n        print(it)\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2*(y[i].item())-1]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n166\n332\n498\n664\n830\n996\n\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[44], line 20\n     18 it = 0\n     19 while loss &gt; 0 or it &gt;=1001:\n---&gt; 20     ax = axarr.ravel()[current_ax]\n     22     # save the old value of w for plotting later\n     23     old_w = torch.clone(p.w)\n\nIndexError: index 6 is out of bounds for axis 0 with size 6"
  },
  {
    "objectID": "posts/perceptron-post/PerceptronPost.html#experiment-three",
    "href": "posts/perceptron-post/PerceptronPost.html#experiment-three",
    "title": "Perceptron Blog Post",
    "section": "",
    "text": "import pandas as pd\n#reading in the data\nbreast_cancer = pd.read_csv(\"C:/Users/liapu/OneDrive/Desktop/Fall 2024/breast-cancer.csv\")\nbreast_cancer.head()\n\n#turning pd data frames into torch tensors\ntarget = breast_cancer['diagnosis'].map({\"M\": 1, \"B\": 0})\ndata = breast_cancer.drop('diagnosis', axis = 1)\ndata = data.drop('id', axis = 1)\n\nX = torch.tensor(data.values, dtype=torch.float32)\ny = torch.tensor(target.values, dtype = torch.float32)\ndata.head()\n\n\n\n\n\n\n\n\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\nsymmetry_mean\nfractal_dimension_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows × 30 columns\n\n\n\n\n# Initialize\ntorch.manual_seed(1234567)\np = Perceptron()\nopt = PerceptronOptimizer(p)\nn = X.shape[0]\n\n# Training loop parameters\nmax_steps = 1000\nloss_vec = []\nit = 0\nloss = p.loss(X, y).item()\n\n# Training loop\nwhile loss &gt; 0 and it &lt; max_steps:\n    i = torch.randint(n, size=(1,))\n    x_i = X[[i], :]\n    y_i = y[i]\n\n    local_loss = p.loss(x_i, y_i).item()\n\n    if local_loss &gt; 0:\n        opt.step(x_i, y_i)\n\n    # Update total loss and iteration counter\n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n    it += 1\n\n    # Print progress every 100 steps\n    if it % 100 == 0:\n        print(f\"Step {it}: loss = {loss:.4f}\")\n\n# Final message\nprint(f\"\\nTraining completed in {it} steps with final loss = {loss:.4f}\")\n\nStep 100: loss = 0.3620\nStep 200: loss = 0.3568\nStep 300: loss = 0.3322\nStep 400: loss = 0.1916\nStep 500: loss = 0.3023\nStep 600: loss = 0.6186\nStep 700: loss = 0.0914\nStep 800: loss = 0.1318\nStep 900: loss = 0.4429\nStep 1000: loss = 0.2127\n\nTraining completed in 1000 steps with final loss = 0.2127\n\n\n\nimport matplotlib.pyplot as plt\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nI don’t beleive this data is linearly separable, since it always seems to be occilating above .9, suggestting that .9 is the best we can do with this algorithm in terms of cross entropy loss."
  }
]