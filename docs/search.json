[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog :D"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lia Smith CSCI 0451 Blog",
    "section": "",
    "text": "Logistic Regression Blog Post\n\n\n\n\n\nA blog post implementing logistic regression.\n\n\n\n\n\nApr 9, 2025\n\n\nLia Smith\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/logisticRegression-post/LogisticModel.html",
    "href": "posts/logisticRegression-post/LogisticModel.html",
    "title": "Logistic Regression Blog Post",
    "section": "",
    "text": "https://github.com/Lia-Smith/github.io/blob/main/posts/logisticRegression-post/logistic.py"
  },
  {
    "objectID": "posts/logisticRegression-post/LogisticModel.html#gradient-descent-with-momentum",
    "href": "posts/logisticRegression-post/LogisticModel.html#gradient-descent-with-momentum",
    "title": "Logistic Regression Blog Post",
    "section": "Gradient Descent with Momentum",
    "text": "Gradient Descent with Momentum\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\ncross_entropy_loss_b = []\nsteps_b = np.arange(1, 101)\n\nfor i in range(100):\n    opt.step(X, y, alpha=0.1, beta=.9)\n    loss_momentum = LR.loss(X, y)\n    cross_entropy_loss_b.append(loss_momentum);\n\n# Plooooottting Both\n#momentum\nplt.plot(steps_b, cross_entropy_loss_b)\nplt.xlabel(\"Step\")\nplt.ylabel(\"Cross Entropy Loss\")\nplt.title(\"Loss Over Iterations with Momentum\")\nplt.grid(True)\nplt.show()\n\n# vanilla\nplt.plot(steps, cross_entropy_loss)\nplt.xlabel(\"Step\")\nplt.ylabel(\"Cross Entropy Loss\")\nplt.title(\"Loss Over Iterations Vanilla\")\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/logisticRegression-post/LogisticModel.html#comparison-of-vanilla-and-momentum-gradient-descent",
    "href": "posts/logisticRegression-post/LogisticModel.html#comparison-of-vanilla-and-momentum-gradient-descent",
    "title": "Logistic Regression Blog Post",
    "section": "Comparison of Vanilla and Momentum Gradient Descent",
    "text": "Comparison of Vanilla and Momentum Gradient Descent\nThis experiment is designed to illustrate the benefits of a more complex gradient descent algorithm. The above graphs illustrate convergence of the momemtum gradient descent occurs much more rapidly than the vanilla gradient descent. Momentum converges in about a 100 steps while vanilla gradient descent converges in about 2000 steps."
  },
  {
    "objectID": "posts/logisticRegression-post/LogisticModel.html#overfitting",
    "href": "posts/logisticRegression-post/LogisticModel.html#overfitting",
    "title": "Logistic Regression Blog Post",
    "section": "Overfitting",
    "text": "Overfitting\n\n# two identicle parameter data sets\nLR_overfit = LogisticRegression() \nopt_overfit = GradientDescentOptimizer(LR_overfit)\nX_train, y_train = classification_data(n_points = 50, noise = 0.5, p_dims = 200)\nX_test, y_test = classification_data(n_points = 50, noise = 0.5, p_dims = 200)\n\nadd_bias(X_test) #add ones column for intercept term\nadd_bias(X_train)\n\nfor i in range(1000):\n    opt_overfit.step(X_train, y_train, alpha=0.1, beta=.9)\n    loss = LR_overfit.loss(X_train,y_train);\n    \n    \ny_hat = 1/(1+torch.exp(-LR_overfit.score(X_train)))\nacc_train = torch.abs(y_hat-y_train-1).mean()\nprint(f\"training accuracy: {acc_train}\")\n\ny_hat = 1/(1+torch.exp(-LR_overfit.score(X_test)))\nacc_test = torch.abs(y_hat-y_test-1).mean()\nprint(f\"testing accuracy : {acc_test}\")\n\ntraining accuracy: 0.9998152256011963\ntesting accuracy : 0.8587407469749451"
  },
  {
    "objectID": "posts/logisticRegression-post/LogisticModel.html#overfitting-experiment",
    "href": "posts/logisticRegression-post/LogisticModel.html#overfitting-experiment",
    "title": "Logistic Regression Blog Post",
    "section": "Overfitting Experiment",
    "text": "Overfitting Experiment\nThis experiment demonstrates the relationship between model complexity and overfitting. As a model becomes more complex, it is likely to capture relationships within the training data that do not generalize to other data. Within this experiment, 150 features yielded 100% accuracy in the training set, but adversely affected the testing data, leading to a lower 83% accuracy."
  },
  {
    "objectID": "posts/logisticRegression-post/LogisticModel.html#introduction-to-the-data",
    "href": "posts/logisticRegression-post/LogisticModel.html#introduction-to-the-data",
    "title": "Logistic Regression Blog Post",
    "section": "Introduction to the Data",
    "text": "Introduction to the Data\nThe Breast Cancer dataset contains variables derived from digitalized images of a fine needle aspirate (FNA) of a breast mass. The features describe characteristics of the cell nuclie present in the image. This data was compiled from Wisconsin patients. The column diagnosis is a categorical variable with two levels, benign and malignant, which describe whether the mass is benign or malignant. Other variables are continuous variables delineating the various features of the digitized images such as worst_radius of the mass, etc. With this data, I aim to create a model to classify the data using logistic regression with momemtum in gradient descent. Unfortunately I was unable to find the cite the data is originally from.\nWolberg, William, W. Street, and Olvi Mangasarian. “Breast Cancer Wisconsin (Prognostic).” UCI Machine Learning Repository, 1995, https://doi.org/10.24432/C5GK50.\n\nimport pandas as pd\n#reading in the data\nbreast_cancer = pd.read_csv(\"C:/Users/liapu/OneDrive/Desktop/Fall 2024/breast-cancer.csv\")\nbreast_cancer.head()\n\n#turning pd data frames into torch tensors\ntarget = breast_cancer['diagnosis'].map({\"M\": 1, \"B\": 0})\ndata = breast_cancer.drop('diagnosis', axis = 1)\ndata = data.drop('id', axis = 1)\n\nX = torch.tensor(data.values, dtype=torch.float32)\ny = torch.tensor(target.values, dtype = torch.float32)\ndata.head()\n\n\n\n\n\n\n\n\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\nsymmetry_mean\nfractal_dimension_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows × 30 columns\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n# 60% train, 20% val, 20% test (using skit learn :O )\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp)\n\nadd_bias(X_train);\nadd_bias(X_test);\nadd_bias(X_val);\n\n\n#no momentum\ncross_entropy_loss = []\nsteps = np.arange(1, 1001)\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nfor i in range(1000):\n    opt.step(X_train, y_train, alpha=0.1, beta=0)\n    loss = LR.loss(X_train, y_train)\n    cross_entropy_loss.append(loss);\n\n\n #momentum\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\ncross_entropy_loss_b = []\nsteps_b = np.arange(1, 1001)\n\nfor i in range(1000):\n    opt.step(X_train, y_train, alpha=0.01, beta=.9)\n    loss = LR.loss(X_train, y_train)\n    cross_entropy_loss_b.append(loss);\n\n\n# Plooooottting Both\n#momentum\nplt.plot(steps_b, cross_entropy_loss_b)\nplt.xlabel(\"Step\")\nplt.ylabel(\"Cross Entropy Loss\")\nplt.title(\"Loss Over Iterations with Momentum\")\nplt.grid(True)\nplt.show()\n\n# vanilla\nplt.plot(steps, cross_entropy_loss)\nplt.xlabel(\"Step\")\nplt.ylabel(\"Cross Entropy Loss\")\nplt.title(\"Loss Over Iterations wthout Momentum\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#validation loss w momentum/ no momentum\n #momentum\nLR_val = LogisticRegression() \nopt_val = GradientDescentOptimizer(LR_val)\n\ncross_entropy_loss_val = []\nsteps_val = np.arange(1, 1001)\n\nfor i in range(1000):\n    opt_val.step(X_val, y_val, alpha=0.01, beta=.9)\n    loss_val = LR_val.loss(X_val, y_val)\n    cross_entropy_loss_val.append(loss_val);\n\n\nLR_val_van = LogisticRegression() \nopt_val_van = GradientDescentOptimizer(LR_val_van)\n\ncross_entropy_loss_val_van = []\nsteps_val_van = np.arange(1, 1001)\n\nfor i in range(1000):\n    opt_val_van.step(X_val, y_val, alpha=0.01, beta=.9)\n    loss_val_van = LR_val_van.loss(X_val, y_val)\n    cross_entropy_loss_val_van.append(loss_val_van);\n\n\n# Plooooottting Both\n#momentum\nplt.plot(steps_val, cross_entropy_loss_val)\nplt.xlabel(\"Step\")\nplt.ylabel(\"Cross Entropy Loss\")\nplt.title(\"Loss Over Iterations with Momentum\")\nplt.grid(True)\nplt.show()\n\n# vanilla\nplt.plot(steps_val_van, cross_entropy_loss_val_van)\nplt.xlabel(\"Step\")\nplt.ylabel(\"Cross Entropy Loss\")\nplt.title(\"Loss Over Iterations wthout Momentum\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#test set evaluation/model loss\nloss = LR.loss(X_test, y_test)\nprint(f\"loss : {loss}\")\n#probabillity\ny_hat = 1/(1+torch.exp(-LR.score(X_test)))\n\n#make 0 or 1 \ny_pred = (y_hat &gt;= 0.5).float()\n\n#check for correct classification + mean for %correct\naccuracy = (y_pred == y_test).float().mean()\n\nprint(f\"accuracy : {accuracy}\")\n\nloss : 0.14749480783939362\naccuracy : 0.9666666388511658"
  },
  {
    "objectID": "posts/logisticRegression-post/LogisticModel.html#abstract",
    "href": "posts/logisticRegression-post/LogisticModel.html#abstract",
    "title": "Logistic Regression Blog Post",
    "section": "Abstract",
    "text": "Abstract\nThis blog posts implements logistic regression with momentum gradient descent. I perform experiments comparing the number of steps momentum and vanilla gradient descent take to converge for a randomly generated dataset and a breast cancer classification data set. Additionally, I experiment with overfitting to understand the affects it has on test data outcomes. Finally, I fit the breast cancer test data to see how well the model performs on the dataset.\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer"
  },
  {
    "objectID": "posts/logisticRegression-post/LogisticModel.html#discussion",
    "href": "posts/logisticRegression-post/LogisticModel.html#discussion",
    "title": "Logistic Regression Blog Post",
    "section": "Discussion",
    "text": "Discussion\nIn this notebook, I utilized my own implementation of logistic regression to compare gradient descent with and without momentum, simulate overfitting to the data, and fit the model to an actual dataset. My model performs extremely well on this dataset, without being too computationally expensive. Logistic regression is a simple machine learning algorithm that performs well on binary classification tasks."
  }
]